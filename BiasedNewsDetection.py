# -*- coding: utf-8 -*-
"""I320D - Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cS55Kk2lWSGSnu4lXFKmQ9vNeWlLbmPt
"""

!pip install datasets --upgrade
from datasets import load_dataset

from huggingface_hub import login

login()

dataset = load_dataset("newsmediabias/news-bias-full-data")

# import basic libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

test_df = dataset["test"].to_pandas()
train_df = dataset["train"].to_pandas()

"""# EDA
Initial Explorations and Visualizations
"""

test_df.head()

print(test_df['text'][0])

# print dimensions
print(test_df.shape)
print(train_df.shape)

# see the different categories distribution for dimension, biased_words, aspect, label, sentiment
print(test_df['dimension'].value_counts())
print(test_df['biased_words'].value_counts())
print(test_df['aspect'].value_counts())
print(test_df['label'].value_counts())
print(test_df['sentiment'].value_counts())

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import ast

# plot pie chart of the dimension categories
def piecharts(train_df, test_df, col, title=''):
    fig, axs = plt.subplots(1, 2, figsize=(12, 5))

    train_counts = train_df[col].value_counts()
    test_counts = test_df[col].value_counts()

    axs[0].pie(train_counts, labels=train_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Set3.colors)
    axs[0].set_title(f'Train - {col}')

    axs[1].pie(test_counts, labels=test_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Set3.colors)
    axs[1].set_title(f'Test - {col}')

    fig.suptitle(f'{title or "Distribution of " + col.capitalize()} (Train vs Test)', fontsize=14)
    plt.tight_layout()
    plt.show()



piecharts(train_df, test_df, 'dimension', 'Dimension')
piecharts(train_df, test_df, 'aspect', 'Aspect')
piecharts(train_df, test_df, 'sentiment', 'Sentiment')

# bar graph of aspects

def barplot(df, split_name):
    plt.figure(figsize=(12, 5))
    sns.countplot(data=df, y='aspect', order=df['aspect'].value_counts().index, palette='Set2')
    plt.title(f'Aspect Distribution - {split_name}')
    plt.xlabel('Count')
    plt.ylabel('Aspect')
    plt.tight_layout()
    plt.show()

# Plot for train and test
barplot(train_df, 'Train')
barplot(test_df, 'Test')

# plot label distribution  for test and train
plt.figure(figsize=(12,5))

plt.subplot(1, 2, 1)
sns.countplot(data=train_df, x='label', order=train_df['label'].value_counts().index)
plt.title('Label Distribution - Train')

plt.subplot(1, 2, 2)
sns.countplot(data=test_df, x='label', order=test_df['label'].value_counts().index)
plt.title('Label Distribution - Test')

plt.tight_layout()
plt.show()

# word cloud of biased_words (in list format)

def wordcloud(df):
    all_words = []
    for item in df['biased_words']:
        if isinstance(item, str) and item.strip():
            words = ast.literal_eval(item)  # safely convert string to list
            all_words.extend(words)

    # Join into one long string
    text = " ".join(all_words)

    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Display
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title("Word Cloud of Biased Words")
    plt.show()

wordcloud(df=train_df)
wordcloud(df=test_df)

"""### "News Media" category"""

news_train = train_df[train_df['aspect'] == 'News Media']
news_test = test_df[test_df['aspect'] == 'News Media']

# plot label distribution  for test and train
plt.figure(figsize=(12,5))

plt.subplot(1, 2, 1)
sns.countplot(data=news_train, x='label', order=train_df['label'].value_counts().index)
plt.title('Label Distribution - Train')

plt.subplot(1, 2, 2)
sns.countplot(data=news_test, x='label', order=test_df['label'].value_counts().index)
plt.title('Label Distribution - Test')

plt.tight_layout()
plt.show()

# word cloud of biased_words (in list format)

def wordcloud(df):
    all_words = []
    for item in df['biased_words']:
        if isinstance(item, str) and item.strip():
            words = ast.literal_eval(item)  # safely convert string to list
            all_words.extend(words)

    # Join into one long string
    text = " ".join(all_words)

    # Generate word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Display
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title("Word Cloud of Biased Words")
    plt.show()

wordcloud(df=news_train)
wordcloud(df=news_test)

print(train_df['label'].value_counts())
print(train_df.info())
print(news_train.info())
print(news_train['label'].value_counts())
print(news_test['label'].value_counts())

"""confirmed i still have enough data to work with!"""

# validation split
from sklearn.model_selection import train_test_split

news_train, news_val = train_test_split(news_train, test_size=0.1, stratify=news_train['label'], random_state=42)

"""# Preprocessing

Normalize strange unicode or HTML (&amp;, etc.)

Replace long URLs or usernames with placeholders (@user, http://link)

Remove emojis
"""

!pip install swifter
import re
import html
import nltk
import swifter
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

nltk.download('punkt_tab')
nltk.download('stopwords')

# Set up stopwords
stop_words = set(stopwords.words('english'))

# Define emoji pattern
emoji_pattern = re.compile("["
    u"\U0001F600-\U0001F64F"  # emoticons
    u"\U0001F300-\U0001F5FF"  # symbols & pictographs
    u"\U0001F680-\U0001F6FF"  # transport & map symbols
    u"\U0001F1E0-\U0001F1FF"  # flags
    u"\U00002500-\U00002BEF"  # chinese/japanese/korean
    "]+", flags=re.UNICODE)

# Preprocessing function
def preprocess_text(text):
    if not isinstance(text, str):
        return []
    text = html.unescape(text)
    text = re.sub(emoji_pattern, '', text)
    text = re.sub(r"http\S+|www\S+", "http://link", text)
    text = re.sub(r"@\w+", "@user", text)
    text = text.lower()
    text = re.sub(r"[^\w\s]", "", text)  # remove punctuation
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Apply to all splits
news_train['tokens'] = news_train['text'].astype(str).swifter.apply(preprocess_text)
news_val['tokens'] = news_val['text'].astype(str).swifter.apply(preprocess_text)
news_test['tokens'] = news_test['text'].astype(str).swifter.apply(preprocess_text)

# preview output it works yayyyyy
print(news_train[['text', 'tokens']].sample(5))

from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

from datasets import Dataset
from transformers import DistilBertTokenizerFast

# Convert DataFrames to Hugging Face Datasets
train_dataset = Dataset.from_pandas(news_train[["text", "label"]])
val_dataset = Dataset.from_pandas(news_val[["text", "label"]])
test_dataset = Dataset.from_pandas(news_test[["text", "label"]])


tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)

# Tokenize datasets
train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Rename 'label' column to 'labels' to work with Hugging Face Trainer
train_dataset = train_dataset.rename_column("label", "labels")
val_dataset = val_dataset.rename_column("label", "labels")
test_dataset = test_dataset.rename_column("label", "labels")

# Set format for PyTorch
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

print(train_dataset.column_names)

"""training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Modeling
"""

from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
import torch

label_map = {"Neutral": 0, "Slightly Biased": 1, "Highly Biased": 2}
for df in [news_train, news_val, news_test]:
    df["label"] = df["label"].map(label_map)

train_dataset = Dataset.from_pandas(news_train[["text", "label"]])
val_dataset = Dataset.from_pandas(news_val[["text", "label"]])
test_dataset = Dataset.from_pandas(news_test[["text", "label"]])

cmodel = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)

# Sample a smaller dataset
small_train_dataset = train_dataset.select(range(500))
small_val_dataset = val_dataset.select(range(100))

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_dataset = train_dataset.rename_column("label", "labels")
val_dataset = val_dataset.rename_column("label", "labels")
test_dataset = test_dataset.rename_column("label", "labels")

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

small_train_dataset = train_dataset.select(range(500))
small_val_dataset = val_dataset.select(range(50))

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_val_dataset,
)

trainer.train()

eval_results = trainer.evaluate()
print(eval_results)

"""Results: After training the model on a 500-sample subset for three epochs, we evaluated its performance on a small validation set. The model achieved an evaluation loss of 0.370, indicating a reasonable generalization ability for a lightweight model trained on limited data. The evaluation process completed in under half a second, with a processing rate of approximately 228 samples per second. This suggests the setup is efficient for iterative development, especially under hardware or time constraints."""

predictions = trainer.predict(small_val_dataset)
print(predictions.metrics)  # Shows accuracy, loss, etc.

"""Results:
We trained a DistilBERT model for 3 epochs on a subset of 500 samples from the "News Media" aspect of the News Bias dataset. The model achieved a training loss of 0.173 and a validation loss of 0.370, suggesting it was able to learn meaningful patterns from the data even within a short training window.

Given the reduced size of the dataset and limited compute resources, these results indicate the model has potential to perform well when scaled up. The training and evaluation were completed in under 1 minute, demonstrating feasibility for rapid prototyping.
"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Get true labels and predictions
preds = predictions.predictions.argmax(axis=1)
true_labels = predictions.label_ids

# 1) classification report (P, R, F1, Accuracy)
print(classification_report(true_labels, preds, target_names=["Neutral", "Slightly Biased", "Highly Biased"]))

# 2) confusion mtrix
cm = confusion_matrix(true_labels, preds)
print("Confusion Matrix:\n", cm)

# 3) AUC-ROC (for multiclass using one vs rest)
from sklearn.preprocessing import label_binarize

y_true_binarized = label_binarize(true_labels, classes=[0,1,2])
y_pred_probs = predictions.predictions  # raw logits or softmax probs

auc = roc_auc_score(y_true_binarized, y_pred_probs, multi_class='ovr')
print(f"AUC-ROC Score: {auc:.3f}")

!pip uninstall -y transformers tokenizers
!rm -rf /usr/local/lib/python3.11/dist-packages/transformers
!rm -rf /usr/local/lib/python3.11/dist-packages/tokenizers

!pip install evaluate
!pip install transformers==4.37.2

# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
from sklearn.preprocessing import label_binarize
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
import evaluate

# Load and filter dataset
dataset = load_dataset("newsmediabias/news-bias-full-data")
train_df = dataset['train'].to_pandas()
train_df = train_df[train_df['aspect'] == 'News Media']
train_df = train_df[train_df['label'].isin(["Neutral", "Slightly Biased", "Highly Biased"])]
train_df = train_df.sample(500, random_state=42).reset_index(drop=True)
label_map = {"Neutral": 0, "Slightly Biased": 1, "Highly Biased": 2}
train_df['label'] = train_df['label'].map(label_map)

# Tokenization
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)
train_dataset = Dataset.from_pandas(train_df[['text', 'label']])
tokenized_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
tokenized_train = tokenized_dataset['train']
tokenized_val = tokenized_dataset['test']

# Model setup
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
accuracy = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return accuracy.compute(predictions=preds, references=labels)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate
predictions = trainer.predict(tokenized_val)
preds = np.argmax(predictions.predictions, axis=1)
true_labels = predictions.label_ids

print("Classification Report:")
print(classification_report(true_labels, preds, target_names=["Neutral", "Slightly Biased", "Highly Biased"]))

# Confusion Matrix
cm = confusion_matrix(true_labels, preds)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Neutral", "Slightly Biased", "Highly Biased"],
            yticklabels=["Neutral", "Slightly Biased", "Highly Biased"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

# AUC-ROC
y_true_binarized = label_binarize(true_labels, classes=[0, 1, 2])
auc = roc_auc_score(y_true_binarized, predictions.predictions, multi_class='ovr')
print(f"\nAUC-ROC Score: {auc:.3f}")

#example
import torch

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

from scipy.special import softmax

# Example headlines
example_texts = [
    "Government denies involvement in foreign conflict",
    "Mainstream media hides the truth about rising crime rates",
    "Studies suggest climate change is exaggerated by left-leaning outlets"
]

# Tokenize and predict
example_tokens = tokenizer(example_texts, return_tensors="pt", padding=True, truncation=True)
model.eval()
with torch.no_grad():
    outputs = model(**example_tokens)

# Get softmax probabilities
probs = softmax(outputs.logits.numpy(), axis=1)

# Map labels
label_map_reverse = {0: "Neutral", 1: "Slightly Biased", 2: "Highly Biased"}

# Print predictions
for text, prob in zip(example_texts, probs):
    pred_label = label_map_reverse[np.argmax(prob)]
    print(f"\nðŸ“° Headline: {text}")
    print(f"â†’ Predicted: {pred_label}")
    print(f"â†’ Scores: Neutral: {prob[0]:.2f}, Slightly: {prob[1]:.2f}, High: {prob[2]:.2f}")